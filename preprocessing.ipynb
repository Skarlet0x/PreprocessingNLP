{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOD/LKP5bwLhYmxngsQiGb2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# TO-DO\n",
        "\n",
        "# EXAMPLES\n",
        "\n",
        "# REMOVE: multiple letters\n",
        "# APPLY: normalization\n",
        "\n",
        "# VISUALIZE:\n",
        "# ideas for exploratory data analysis??\n",
        "\n",
        "# PIPELINE !!!!"
      ],
      "metadata": {
        "id": "lof4DR51iRPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORTS"
      ],
      "metadata": {
        "id": "Hvdartlkjk3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "glJbHLFyjl4A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLEAN"
      ],
      "metadata": {
        "id": "z-og08azjW0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# everything is substituted with a whitespace, rather than fully deleted to prevent accidental mergers\n",
        "# this is later dealt with by removing the resulting multiple whitespaces"
      ],
      "metadata": {
        "id": "fFmmulL4qesl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_html(text):    \n",
        "  return BeautifulSoup(text, \"lxml\").get_text()"
      ],
      "metadata": {
        "id": "uxxSSHPqvVCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MGcBITQviPUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f503af3-a6a0-4cec-d068-d461482f72cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "all_stopwords = stopwords.words(\"english\")\n",
        "\n",
        "# negation is important, don't even know why it's considered a stopword\n",
        "all_stopwords.remove(\"not\")\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  # tokenize the text\n",
        "  text = text.split()\n",
        "  # checked and removed\n",
        "  text = [word for word in text if not word in all_stopwords]\n",
        "  # joined back to a single string\n",
        "  text = \" \".join(text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_tags(text):\n",
        "  # @ + name, but preserves emails\n",
        "  return re.sub(r\"^@[A-Za-z0-9]{1,}|\\s@[A-Za-z0-9]{1,}\", \" \", text)"
      ],
      "metadata": {
        "id": "nnYMUDvCmGvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_hashtags(text):\n",
        "  # deletes the whole hashtag, if the content is to be preserved, using only remove_punctuation does the trick\n",
        "  return re.sub(r\"#[A-Za-z0-9]+\", \" \", text)"
      ],
      "metadata": {
        "id": "y87I2kBbLq8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_links(text):\n",
        "  # http or https\n",
        "  return re.sub(r\"https?://[A-Za-z0-9./]+\", \" \", text)"
      ],
      "metadata": {
        "id": "_AOKGyBImQ1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):   \n",
        "  # except '\n",
        "  return re.sub(r\"[^A-Za-z']\", \" \", text)"
      ],
      "metadata": {
        "id": "vMxHbtYXm4p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emails(text):\n",
        "  # name_surname00@domain.co.uk\n",
        "  return re.sub(r\"[a-z0-9+_.-]+@[a-z0-9+_.-]+.[a-z0-9+_-]+.[a-z]+\", \" \", text)"
      ],
      "metadata": {
        "id": "2pi1rtVLnQpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO-DO - handle exceptions for legit words with double letters\n",
        "\n",
        "def remove_doubles(text):\n",
        "  # recurring 1+ times, substituted by it ocurring once\n",
        "  return re.sub(r\"([A-Za-z])\\1+\", r\"\\1\", text)"
      ],
      "metadata": {
        "id": "fsRIezL9q2BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_whitespaces(text):\n",
        "  return re.sub(r\" +\", \" \", text)"
      ],
      "metadata": {
        "id": "E0GzVn1onDD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PROCESS"
      ],
      "metadata": {
        "id": "jYzlVFQ3jaeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_spelling(text):\n",
        "\treturn TextBlob(text).correct()"
      ],
      "metadata": {
        "id": "AcllOquekXiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO-DO - normalization"
      ],
      "metadata": {
        "id": "hMMOjnZaENEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def stem(text):\n",
        "  text = text.split()\n",
        "  return \" \".join([porter.stem(word) for word in text])"
      ],
      "metadata": {
        "id": "pWkXhs47wfhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# avoids errors ??\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "  # Map POS tag to first character accepted by lemmatize()\n",
        "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "  tag_dict = {\"J\": wordnet.ADJ,\n",
        "              \"N\": wordnet.NOUN,\n",
        "              \"V\": wordnet.VERB,\n",
        "              \"R\": wordnet.ADV}\n",
        "\n",
        "  return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def lemmatize(text):\n",
        "  text = text.split()\n",
        "  return \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in text])"
      ],
      "metadata": {
        "id": "dfRGbCaixy3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANALYZE"
      ],
      "metadata": {
        "id": "1bcRrVwqtzl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(text):\n",
        "  # tokenizes the text\n",
        "  words = text.split()\n",
        "  return len(words)"
      ],
      "metadata": {
        "id": "ZeoA5qhPNZSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words_unique(text):\n",
        "  # tokenizes the text\n",
        "  words = text.split()\n",
        "  # set removes the duplicates\n",
        "  return len(set(words))"
      ],
      "metadata": {
        "id": "AsP58ZeQOUXo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_stopwords(text):\n",
        "  # tokenizes the text\n",
        "  text = text.split()\n",
        "  # collects all stopwords\n",
        "  stopwords = [word for word in text if word in all_stopwords]\n",
        "  return len(stopwords)"
      ],
      "metadata": {
        "id": "XGZrCN1BNB1I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_stopwords_unique(text):\n",
        "  # tokenizes the text\n",
        "  text = text.split()\n",
        "  # collects all stopwords\n",
        "  stopwords = [word for word in text if word in all_stopwords]\n",
        "  # set removes the duplicates\n",
        "  return len(set(stopwords))"
      ],
      "metadata": {
        "id": "cAUTYAwnOYlQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_hashtags(text):\n",
        "  # will also count expressions such as \"he is my #1\", but trying to exclude that would also exclude hashtags like #2022\n",
        "  hashtags = re.findall(r\"#[A-Za-z0-9]+\", text)\n",
        "  return len(hashtags)"
      ],
      "metadata": {
        "id": "X4MZu2T_wYVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_hashtags_unique(text):\n",
        "  # will also count expressions such as \"he is my #1\", but trying to exclude that would also exclude hashtags like #2022\n",
        "  hashtags = re.findall(r\"#[A-Za-z0-9]+\", text)\n",
        "  # set removes the duplicates\n",
        "  return len(set(hashtags))"
      ],
      "metadata": {
        "id": "jfk77FWOprLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tags(text):\n",
        "  # @ + name\n",
        "  tags = re.findall(r\"^@[A-Za-z0-9]{1,}|\\s@[A-Za-z0-9]{1,}\", text)\n",
        "  return len(tags)"
      ],
      "metadata": {
        "id": "MAnrN7vwNCDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_characters_without_spaces(text):\n",
        "  # removes the spaces\n",
        "  text = text.replace(\" \", \"\")\n",
        "  return len(text)"
      ],
      "metadata": {
        "id": "iJlFCTE0RRz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_characters_with_spaces(text):\n",
        "  return len(text)"
      ],
      "metadata": {
        "id": "sX6vvnrWRkHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_word_len(text):\n",
        "  # separates the words\n",
        "  words = text.split()\n",
        "  # gets the number of characters\n",
        "  lengths = [len(word) for word in words]\n",
        "  # avg\n",
        "  return sum(lengths) / len(lengths)"
      ],
      "metadata": {
        "id": "K3kBNq-sSuUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_sent_len(text):\n",
        "  # separates the sentences\n",
        "  sents = nltk.sent_tokenize(text)\n",
        "  # gets the number of words\n",
        "  lengths = [len(sent.split()) for sent in sents]\n",
        "  # avg\n",
        "  return sum(lengths) / len(lengths)"
      ],
      "metadata": {
        "id": "KajTgZm-S1Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VISUALIZE"
      ],
      "metadata": {
        "id": "sm44Xxr77QoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l7KgJ2mV7SiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EXTRACT"
      ],
      "metadata": {
        "id": "XYTS46i3wppE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_emails(text):\n",
        "  # name_surname00@domain.co.uk\n",
        "  emails = re.findall(r\"[a-z0-9+_.-]+@[a-z0-9+_.-]+.[a-z0-9+_-]+.[a-z]+\", text)\n",
        "  return emails"
      ],
      "metadata": {
        "id": "VSyDY1Tyy0z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tags(text):\n",
        "  # # @ + name, but avoids fetching emails\n",
        "  tags = re.findall(r\"^@[A-Za-z0-9]{1,}|\\s@[A-Za-z0-9]{1,}\", text)\n",
        "  # the regex returns a space before the tag for some reason which needs to be removed\n",
        "  tags = [tag.replace(\" \", \"\") for tag in tags]\n",
        "  return tags"
      ],
      "metadata": {
        "id": "b5wgQ__EEZUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hashtags(text):\n",
        "  # will also fetch expressions such as \"he is my #1\", but trying to exclude that would also exclude hashtags like #2022\n",
        "  hashtags = re.findall(r\"#[A-Za-z0-9]+\", text)\n",
        "  return hashtags"
      ],
      "metadata": {
        "id": "zR2C409CEZSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_URLs(text):\n",
        "  # http or https\n",
        "  URLs = re.findall(r\"https?://[A-Za-z0-9./]+\", text)\n",
        "  return URLs"
      ],
      "metadata": {
        "id": "Z8jx8iCCEZP8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}